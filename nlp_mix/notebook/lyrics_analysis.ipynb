{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP mix: ritmos distintos que dicen lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import json\n",
    "import pandas as pd\n",
    "import lyricsgenius as lg\n",
    "from tqdm import tqdm\n",
    "from pysentimiento import create_analyzer\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import yake\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor(lan=\"es\", top=5)\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "tagger = SequenceTagger.load(\"flair/ner-spanish-large\")\n",
    "ner = pipeline(\"ner\", model=\"dccuchile/bert-base-spanish-wwm-uncased\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songs_by_artist(artist_name: str, max_songs: int = 40) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Fetches song titles and lyrics for a given artist using the Genius API.\n",
    "\n",
    "    Args:\n",
    "        artist_name (str): The name of the artist to search for.\n",
    "        max_songs (int): The maximum number of songs to retrieve. Defaults to 40.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str], List[str]]: Lists of artist names, song titles, and lyrics.\n",
    "    \"\"\"\n",
    "    genius.skip_non_songs = True\n",
    "    genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "    genius.remove_section_headers = True\n",
    "    genius.verbose = False\n",
    "\n",
    "    artist_obj = genius.search_artist(artist_name, max_songs=max_songs, sort=\"popularity\")\n",
    "\n",
    "    artist_list = []\n",
    "    title_list = []\n",
    "    lyrics_list = []\n",
    "\n",
    "    for song in artist_obj.songs:\n",
    "        artist_list.append(song.artist)\n",
    "        title_list.append(song.title)\n",
    "        try:\n",
    "            lyrics_list.append(song.lyrics)\n",
    "        except Exception:\n",
    "            lyrics_list.append(\"Lyrics not found\")\n",
    "\n",
    "    return artist_list, title_list, lyrics_list\n",
    "\n",
    "\n",
    "def clean_text_from_keywords(text: str, keywords: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Cleans text by iteratively trimming content before each keyword found.\n",
    "    Keeps only the text after the last matched keyword.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "        keywords (List[str]): A list of lowercase keywords to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after the last found keyword.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    for word in keywords:\n",
    "        if word in text_lower:\n",
    "            parts = text_lower.split(word, 1)\n",
    "            text = parts[1]  # Keep only the part after the keyword\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(credentials_path, 'r') as file:\n",
    "    credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keyLlyrics = credentials[\"api_genius\"][\"acces_token\"]\n",
    "genius = lg.Genius(api_keyLlyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_artists = [\"Andrea echeverri\",\"Bad Bunny\",\"Dario Gomez\", \"Diomedes Diaz\",\"Soda Stereo\",\n",
    "            \"Mercedes Soza\", \"Helenita Vargas\", \"Ismael Rivera\",\"Shakira\",\n",
    "            \"Karol g\",\"Eladio Carrion\", \"Celia Cruz\",\"Willie Colon\",\n",
    "            \"Garzón y Collazos\",\"Julio Jaramillo\",\"El caballero Gaucho\",\"Miguel Mateos\",\n",
    "             \"Miriam Hernández\",\"Young Miko\",\"Kany Garcia\",\"Natalia Jiménez\"\n",
    "            \"ekhymosis\",\"Sergio Vargas\", \"Romeo Santos\",\"Silvio Rodrigez\",\n",
    "            \"alcolirykoz\",\"Penyair\",\"Angela Aguilar\",\"La India\",\"ivy queen\",\n",
    "            \"proyecto uno\",\"Javier Solis\",\"el alfa\",\"Peso Pluma\", \n",
    "            \"Fuerza Regida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist_result = pd.DataFrame()\n",
    "for artist in tqdm(l_artists):\n",
    "    df_artist = pd.DataFrame()\n",
    "    l_artist, l_titles, l_letters = get_songs_by_artist(artist,max_song=60)\n",
    "    df_artist[\"artist\"] = [artist]*len(l_letters)\n",
    "    df_artist[\"title\"] = l_titles\n",
    "    df_artist[\"letter\"] = l_letters\n",
    "    df_artist_result = pd.concat([df_artist_result,df_artist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist_result.to_pickle(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_verses_by_author(df: pd.DataFrame, lyrics_column: str, group_size: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Splits song lyrics into groups of verses and associates each group with the original artist and title.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame with at least 'artist', 'title', and the lyrics column.\n",
    "        lyrics_column (str): The name of the column that contains the song lyrics.\n",
    "        group_size (int): The number of verses to concatenate per group. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with columns 'artist', 'title', and 'verse_group',\n",
    "                      where each row contains a group of `group_size` verses.\n",
    "    \"\"\"\n",
    "    artists, titles, grouped_verses = [], [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        artist = row[\"artist\"]\n",
    "        title = row[\"title\"]\n",
    "        lyrics = row[lyrics_column]\n",
    "\n",
    "        # Split lyrics into clean, non-empty lines\n",
    "        verses = [v.strip() for v in str(lyrics).split('\\n') if v.strip()]\n",
    "\n",
    "        # Group verses\n",
    "        for i in range(0, len(verses), group_size):\n",
    "            group = verses[i:i + group_size]\n",
    "            combined = \" \".join(group)\n",
    "            artists.append(artist)\n",
    "            titles.append(title)\n",
    "            grouped_verses.append(combined)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"artist\": artists,\n",
    "        \"title\": titles,\n",
    "        \"verse_group\": grouped_verses\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verses_4 = group_verses_by_author(df_artist_result,\"clean_lyrics\",4)\n",
    "df_verses_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos y emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_sen = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "analyzer_emo = create_analyzer(task=\"emotion\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    result_sen = analyzer_sen.predict(text)\n",
    "    proba = result_sen.probas\n",
    "    return proba['NEG'],proba['NEU'],proba['POS'], result_sen.output\n",
    "def emotion_analysis(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    result_emo = analyzer_emo.predict(text)\n",
    "    proba = result_emo.probas\n",
    "    return proba['joy'],proba['surprise'],proba['sadness'],proba['disgust'],proba['fear'],proba['anger'],proba['others'], result_emo.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verses_4[['chorus_neg', 'chorus_neu', 'chorus_pos','chorus_sen']] = df_verses_4['verse_group'].apply(lambda x: pd.Series(sentiment_analysis(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verses_4[['letter_joy', 'letter_surprise', \n",
    "            'letter_sadness','letter_disgust',\n",
    "            'letter_fear','letter_anger',\n",
    "            'letter_others','letter_emo'\n",
    "            ]] = df_verses_4['verse_group'].apply(lambda x: pd.Series(emotion_analysis(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyw_yake(text):\n",
    "    l_k_words = []\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for palabra, score in keywords:\n",
    "        l_k_words.append(palabra)\n",
    "    return l_k_words\n",
    "def keyw_rank(text):\n",
    "    l_k_words = []\n",
    "    doc = nlp(text)\n",
    "    for phrase in doc._.phrases[:5]:\n",
    "        l_k_words.append(phrase.text)\n",
    "    return l_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verses_4['keyw_yake']=df_verses_4['verse_group_clean'].apply(keyw_yake)\n",
    "df_verses_4['keyw_rank']=df_verses_4['verse_group_clean'].apply(keyw_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and embeddings calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener combinaciones únicas\n",
    "comb_sen = df_verses_4[['chorus_sen', 'letter_emo']].drop_duplicates()\n",
    "\n",
    "# Crear sub-dataframes en un diccionario\n",
    "sub_dataframes = {\n",
    "    (fila['chorus_sen'], fila['letter_emo']): df_verses_4[\n",
    "        (df_verses_4['chorus_sen'] == fila['chorus_sen']) & (df_verses_4['letter_emo'] == fila['letter_emo'])\n",
    "    ]\n",
    "    for _, fila in comb_sen.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_cluster = {}\n",
    "for comb in tqdm(sub_dataframes.keys()):\n",
    "    df_sub_lyrycs = sub_dataframes[comb].reset_index()\n",
    "    embeddings = model.encode(df_sub_lyrycs['verse_group'])\n",
    "    inertia = []\n",
    "    K = range(1, int(len(df_sub_lyrycs)/20))\n",
    "    try:\n",
    "        for k in tqdm(K):\n",
    "            mbk = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=128)\n",
    "            mbk.fit(embeddings)\n",
    "            inertia.append(mbk.inertia_)\n",
    "\n",
    "        # Calcular segunda derivada aproximada para detectar el \"codo\"\n",
    "        inertia_diff = np.diff(inertia)\n",
    "        inertia_diff2 = np.diff(inertia_diff)\n",
    "        k_opt = np.argmin(inertia_diff2) + 2  # +2 porque diff reduce longitud y empieza en k=3\n",
    "        size_cluster[comb]=k_opt\n",
    "    except:\n",
    "        size_cluster[comb]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cluster = []\n",
    "dict_embeddings = {}\n",
    "dict_labels = {}\n",
    "for comb in tqdm(size_cluster.keys()):\n",
    "    k=size_cluster[comb]\n",
    "    df_cluster = sub_dataframes[comb].reset_index()\n",
    "    embeddings = model.encode(df_cluster['verse_group'])\n",
    "    dict_embeddings[comb]=embeddings\n",
    "    if k == 0:\n",
    "        labels_ = [0]*len(df_cluster)\n",
    "    else:\n",
    "        clustering = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=128)\n",
    "        labels_ = clustering.fit_predict(embeddings)\n",
    "    dict_labels[comb]=labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in tqdm(sub_dataframes.keys()):\n",
    "    sub_dataframes[comb]['cluster_label']=dict_labels[comb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dataframes = {}\n",
    "drop_col = ['chorus_neg','chorus_neu','chorus_pos','letter_joy','letter_surprise','letter_sadness','letter_disgust',\n",
    "            'letter_fear','letter_anger','letter_others','keyw_bert','keyw_yake','keyw_rank']\n",
    "for comb in sub_dataframes.keys():\n",
    "    sub_dataframes[comb]['key_words'] = sub_dataframes[comb]['keyw_yake'] + sub_dataframes[comb]['keyw_rank']\n",
    "    clusters_dataframes[comb] = sub_dataframes[comb].drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_from_artis(dict_df, artist):\n",
    "    artist_combs = []\n",
    "    for comb in dict_df.keys():\n",
    "        if artist in dict_df[comb]['artist'].unique():\n",
    "            artist_combs.append(comb)\n",
    "    return artist_combs\n",
    "def title_from_comb_artist(dict_df, artist, comb):\n",
    "    df = dict_df[comb]\n",
    "    df_artist = df[df['artist']==artist]\n",
    "\n",
    "    if len(df_artist)==0:\n",
    "        print('Not Artist for comb')\n",
    "    else:\n",
    "        return df_artist['title'].unique()\n",
    "def verse_from_attributes(dict_df,artist,comb,title):\n",
    "    df = dict_df[comb]\n",
    "    df_filter = df[(df['artist']==artist)&(df['title']==title)]\n",
    "    if len(df_filter)==0:\n",
    "        print('Not verse for comb')\n",
    "    else:\n",
    "        return df_filter['verse_group'].unique()\n",
    "def random_verse_from_attributes(dict_df, artist, comb, title):\n",
    "    df = dict_df[comb]\n",
    "    df_filter = df[(df['artist'] == artist) & (df['title'] == title)]\n",
    "    \n",
    "    if df_filter.empty:\n",
    "        print('No verses found for this combination.')\n",
    "        return None\n",
    "    else:\n",
    "        verses = df_filter['verse_group'].dropna().unique()\n",
    "        if len(verses) == 0:\n",
    "            print('No verses available.')\n",
    "            return None\n",
    "        return random.choice(verses)\n",
    "def att_from_verse(dict_df,verse):\n",
    "    att = {}\n",
    "    for comb in dict_df.keys():\n",
    "        df = dict_df[comb]\n",
    "        if verse in df['verse_group'].unique():\n",
    "            df_verse = df[df['verse_group']==verse].reset_index()\n",
    "            att = dict(df_verse.iloc[0])\n",
    "    return att  \n",
    "def cluster_from_verse(dict_df,verse):\n",
    "    df_cluster = pd.DataFrame()\n",
    "    for comb in dict_df.keys():\n",
    "        df = dict_df[comb]\n",
    "        if verse in df['verse_group'].unique():\n",
    "            df_verse = df[df['verse_group']==verse].reset_index()\n",
    "            cluster_verse = df_verse.iloc[0]['cluster_label']\n",
    "            df_cluster = df[(df['cluster_label']==cluster_verse)&\n",
    "                            (df['verse_group']!=verse)]\n",
    "    return df_cluster.drop_duplicates(subset=['verse_group'])\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union else 0\n",
    "def top_k_jaccard(df, keywords_column, query_keywords, k=5):\n",
    "    query_set = set(query_keywords)\n",
    "\n",
    "    # Calcular distancia de Jaccard para cada fila\n",
    "    distances = []\n",
    "    for keywords in df[keywords_column]:\n",
    "        row_set = set(keywords)\n",
    "        distance = jaccard_distance(query_set, row_set)\n",
    "        distances.append(distance)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"jaccard_distance\"] = distances\n",
    "\n",
    "    # Ordenar por menor distancia y retornar top-k\n",
    "    return df_copy.sort_values(\"jaccard_distance\").head(k).reset_index(drop=True)\n",
    "def top_k_cosine_from_keywords(df, keywords_column, query_keywords, k=5):\n",
    "    # Convertir lista de keywords a texto (espacios entre palabras)\n",
    "    df_keywords_text = df[keywords_column].apply(lambda x: ' '.join(x))\n",
    "    query_text = ' '.join(query_keywords)\n",
    "\n",
    "    # Crear matriz TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df_keywords_text.tolist() + [query_text])\n",
    "\n",
    "    # Última fila es la query\n",
    "    query_vector = tfidf_matrix[-1]\n",
    "    data_matrix = tfidf_matrix[:-1]\n",
    "\n",
    "    # Calcular distancias de coseno\n",
    "    distances = cosine_distances(data_matrix, query_vector).flatten()\n",
    "\n",
    "    # Crear nuevo DataFrame con distancias\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"cosine_distance\"] = distances\n",
    "\n",
    "    # Ordenar por menor distancia\n",
    "    return df_copy.sort_values(\"cosine_distance\").head(k).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_from_artis(clusters_dataframes,'Diomedes Diaz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dataframes[('NEU', 'fear')]['artist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_from_comb_artist(clusters_dataframes,'Diomedes Diaz',('NEG', 'sadness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_proof = random_verse_from_attributes(clusters_dataframes,'Ismaie',('NEG', 'anger'),'No Me Toquen Ese Vals')\n",
    "data_verse = att_from_verse(clusters_dataframes,verse_proof)\n",
    "df_cluster_verse = cluster_from_verse(clusters_dataframes,verse_proof)\n",
    "df_top_jacc = top_k_jaccard(df_cluster_verse,\"key_words\", data_verse['key_words'], k=5)\n",
    "df_top_emb = top_k_cosine_from_keywords(df_cluster_verse,\"key_words\", data_verse['key_words'], k=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
